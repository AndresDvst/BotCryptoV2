Necesito modificar el servicio de IA para agregar soporte de Ollama local con 3 modelos, MANTENIENDO la configuraciÃ³n existente de Gemini, OpenRouter y HuggingFace como fallbacks.

CONTEXTO:
- Tengo un VPS con Ollama corriendo en localhost:11434
- 3 modelos instalados: qwen2.5:7b, deepseek-coder:6.7b, llama3.2:3b
- Quiero que Ollama sea PRIMARIO en VPS, pero mantener las APIs cloud como respaldo
- El bot debe funcionar tanto en VPS (Ollama local) como en PC (Ollama remoto vÃ­a tÃºnel SSH)

ARCHIVOS A MODIFICAR:

1. config/config.py
2. services/ai_analyzer_service.py
3. tests/test_ai_connection.py (crear nuevo test)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 1: config/config.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AGREGAR despuÃ©s de las detecciÃ³n de entorno (lÃ­nea ~50):

    # ========== DETECCIÃ“N DE ENTORNO ==========
    IS_DOCKER = os.getenv('DOCKER_ENV', 'false').lower() in ('1', 'true', 'yes') or os.path.exists('/.dockerenv')
    IS_LINUX = os.name == 'posix'
    IS_WINDOWS = os.name == 'nt'
    IS_VPS = IS_LINUX and not IS_DOCKER

AGREGAR antes de la secciÃ³n de Google Gemini (mantener APIs existentes):

    # ========== OLLAMA (NUEVO - PRIORIDAD 1 EN VPS) ==========
    _env_ollama_host = os.getenv('OLLAMA_HOST', '').strip()
    
    if IS_VPS:
        # En VPS: SIEMPRE usar localhost
        OLLAMA_HOST = "http://localhost:11434"
    elif _env_ollama_host:
        # En PC local: usar IP del VPS del .env
        OLLAMA_HOST = _env_ollama_host
    else:
        # Default: localhost
        OLLAMA_HOST = "http://localhost:11434"
    
    # Modelos Ollama disponibles (orden de prioridad)
    OLLAMA_MODELS = [
        {
            'id': 'qwen2.5:7b',
            'name': 'Qwen 2.5 7B',
            'priority': 1,
            'context_limit': 32768,
            'use_case': 'general',
            'description': 'Modelo principal para anÃ¡lisis general'
        },
        {
            'id': 'deepseek-coder:6.7b',
            'name': 'DeepSeek Coder 6.7B',
            'priority': 2,
            'context_limit': 16384,
            'use_case': 'code',
            'description': 'Especializado en anÃ¡lisis tÃ©cnico y cÃ³digo'
        },
        {
            'id': 'llama3.2:3b',
            'name': 'Llama 3.2 3B',
            'priority': 3,
            'context_limit': 8192,
            'use_case': 'fast',
            'description': 'Modelo rÃ¡pido para respuestas simples'
        }
    ]
    
    OLLAMA_DEFAULT_MODEL = 'qwen2.5:7b'
    OLLAMA_HEALTH_CACHE_SECONDS = int(os.getenv('OLLAMA_HEALTH_CACHE_SECONDS', '60'))
    
    # ========== GOOGLE GEMINI (MANTENER) ==========
    GOOGLE_GEMINI_API_KEY = os.getenv('GOOGLE_GEMINI_API_KEY')
    
    # ========== HUGGING FACE (MANTENER) ==========
    HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY') or os.getenv('HUGGIN_FACE_API_KEY')
    
    # ========== OPENAI (MANTENER) ==========
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or os.getenv('GOOGLE_GPT_API_KEY')
    
    # ========== OPENROUTER (MANTENER) ==========
    OPENROUTER_API_KEY = os.getenv('GOOGLE_OPENROUTER_API_KEY') or os.getenv('OPENROUTER_API_KEY')

AGREGAR mÃ©todo helper al final de la clase:

    @classmethod
    def get_ollama_host(cls) -> str:
        """
        Determina el host de Ollama basÃ¡ndose en el entorno.
        VPS: localhost:11434
        PC: IP del VPS desde .env
        """
        if cls.IS_VPS:
            return "http://localhost:11434"
        return cls.OLLAMA_HOST

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 2: services/ai_analyzer_service.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BUSCAR la secciÃ³n de configuraciÃ³n de Ollama (lÃ­nea ~140) y AGREGAR:

        # ========== CONFIGURACIÃ“N OLLAMA (3 MODELOS - PRIORIDAD 1) ==========
        try:
            ollama_host_raw = Config.get_ollama_host() if hasattr(Config, 'get_ollama_host') else getattr(Config, "OLLAMA_HOST", "")
        except:
            ollama_host_raw = getattr(Config, "OLLAMA_HOST", "")

        self.ollama_host = self._format_ollama_host(ollama_host_raw)
        
        # Cargar configuraciÃ³n de modelos desde Config
        self.ollama_models = getattr(Config, "OLLAMA_MODELS", [
            {'id': 'qwen2.5:7b', 'name': 'Qwen 2.5 7B', 'priority': 1, 'context_limit': 32768, 'use_case': 'general'},
            {'id': 'deepseek-coder:6.7b', 'name': 'DeepSeek Coder 6.7B', 'priority': 2, 'context_limit': 16384, 'use_case': 'code'},
            {'id': 'llama3.2:3b', 'name': 'Llama 3.2 3B', 'priority': 3, 'context_limit': 8192, 'use_case': 'fast'}
        ])
        
        self.ollama_current_model_index = 0
        self.ollama_model = self.ollama_models[0]['id']
        
        self._ollama_health_cache_seconds = int(getattr(Config, "OLLAMA_HEALTH_CACHE_SECONDS", 60) or 60)
        self._ollama_health_last_ts = 0.0
        self._ollama_health_last_ok = False
        
        if self.ollama_host:
            self._providers["ollama"] = "ollama"
            logger.info(f"ğŸ¦™ Ollama configurado: {self.ollama_host}")
            logger.info(f"   ğŸ“¦ Modelos disponibles: {len(self.ollama_models)}")
            for model in self.ollama_models:
                logger.info(f"      - {model['name']} (prioridad: {model['priority']}, {model['use_case']})")
            
            is_vps = getattr(Config, "IS_VPS", False)
            if is_vps:
                logger.info("   ğŸ“ VPS detectado - Ollama es PRIORIDAD 1")
        
        # MANTENER configuraciÃ³n de Gemini, OpenRouter, HuggingFace existente...

MODIFICAR el mÃ©todo _call_ollama para soportar model_id:

    def _call_ollama(self, prompt: str, max_tokens: int, allow_short: bool = False, model_id: Optional[str] = None) -> str:
        """Llama a Ollama con modelo especÃ­fico o actual"""
        host = self._format_ollama_host(self.ollama_host)
        if not host:
            raise RuntimeError("Ollama no configurado")
        
        model_to_use = model_id if model_id else self.ollama_model
        
        payload: Dict[str, Any] = {
            "model": model_to_use,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "options": {"num_predict": max(1, int(max_tokens))},
        }
        
        resp = requests.post(f"{host}/api/chat", json=payload, timeout=self._http_timeout)
        
        if resp.status_code != 200:
            raise RuntimeError(f"Ollama fallÃ³ (HTTP {resp.status_code}) con modelo {model_to_use}")
        
        data = resp.json() or {}
        text = ""
        msg = data.get("message")
        if isinstance(msg, dict):
            text = msg.get("content") or ""
        if not text and isinstance(data.get("response"), str):
            text = data.get("response") or ""
        if not text and not allow_short:
            raise RuntimeError(f"Respuesta vacÃ­a de Ollama (modelo: {model_to_use})")
        
        return text

MODIFICAR _get_provider_priority_list para incluir Ollama con 3 modelos:

    def _get_provider_priority_list(self) -> List[str]:
        """Lista priorizada: Ollama (3 modelos) â†’ Gemini â†’ OpenRouter â†’ HuggingFace"""
        available: List[str] = []
        
        # Ollama con 3 modelos (PRIORIDAD 1 en VPS)
        if self._ollama_health_ok():
            for i, model_config in enumerate(self.ollama_models):
                available.append(f"ollama_{i}")  # ollama_0, ollama_1, ollama_2
        
        # Proveedores cloud (fallbacks)
        if self.gemini_client:
            available.append("gemini")
        if self.openrouter_client:
            available.append("openrouter")
        if self.huggingface_api_key:
            available.append("huggingface")
        
        if not available:
            return []
        
        providers: List[str] = []
        
        with self._state_lock:
            last_success = self._last_success_provider
            active = self.active_provider
        
        if last_success in available:
            providers.append(last_success)
        elif active in available:
            providers.append(active)
        
        # Orden por defecto: Ollama (3 modelos) primero
        default_order = [f"ollama_{i}" for i in range(len(self.ollama_models))] + ["gemini", "openrouter", "huggingface"]
        providers.extend(p for p in default_order if p in available and p not in providers)
        providers.extend(p for p in available if p not in providers)
        
        return providers

MODIFICAR _call_provider para manejar ollama_0, ollama_1, ollama_2:

    def _call_provider(self, provider: str, prompt: str, max_tokens: int = 2048) -> Tuple[str, str]:
        """Llama a proveedor especÃ­fico (con soporte multi-modelo Ollama)"""
        start = time.time()
        with self._state_lock:
            self._metrics["requests"][provider] += 1

        def _call() -> Tuple[str, str]:
            # SOPORTE MULTI-MODELO OLLAMA
            if provider.startswith("ollama_"):
                if not self.ollama_host:
                    raise RuntimeError("Ollama no configurado")
                
                try:
                    model_index = int(provider.split("_")[1])
                    model_config = self.ollama_models[model_index]
                except (IndexError, ValueError):
                    raise RuntimeError(f"Ãndice de modelo Ollama invÃ¡lido: {provider}")
                
                model_id = model_config['id']
                model_name = model_config['name']
                
                logger.info(f"ğŸ¦™ Probando Ollama: {model_name}")
                
                text = self._call_ollama(prompt, max_tokens=max_tokens, allow_short=False, model_id=model_id)
                
                with self._state_lock:
                    self.ollama_current_model_index = model_index
                    self.ollama_model = model_id
                
                logger.info(f"âœ… Ã‰xito con Ollama: {model_name}")
                return text, model_name
            
            # MANTENER cÃ³digo existente para Gemini, OpenRouter, HuggingFace
            if provider == "gemini":
                # ... cÃ³digo existente sin cambios ...
                pass
            
            if provider == "openrouter":
                # ... cÃ³digo existente sin cambios ...
                pass
            
            if provider == "huggingface":
                # ... cÃ³digo existente sin cambios ...
                pass
            
            raise RuntimeError(f"Proveedor desconocido: {provider}")
        
        result = self._run_with_timeout(_call, timeout_seconds=self._timeout)
        
        if isinstance(result, Exception):
            with self._state_lock:
                self._metrics["failures"][provider] += 1
            raise result
        
        elapsed = time.time() - start
        with self._state_lock:
            self._metrics["total_time"][provider] += elapsed
        
        text, model = result
        return str(text), model

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 3: tests/test_ai_connection.py (NUEVO ARCHIVO)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Crear archivo tests/test_ai_connection.py:

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from services.ai_analyzer_service import AIAnalyzerService
from utils.logger import logger

def test_ollama_models():
    """Prueba los 3 modelos de Ollama"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª TEST: OLLAMA (3 MODELOS)")
    logger.info("="*60)
    
    service = AIAnalyzerService()
    
    if not service.ollama_host:
        logger.error("âŒ Ollama no configurado")
        return False
    
    logger.info(f"ğŸ¦™ Host: {service.ollama_host}")
    logger.info(f"ğŸ“¦ Modelos a probar: {len(service.ollama_models)}")
    
    test_prompt = "Explica quÃ© es recursiÃ³n en programaciÃ³n en 2 lÃ­neas"
    
    for i, model_config in enumerate(service.ollama_models):
        model_id = model_config['id']
        model_name = model_config['name']
        
        logger.info(f"\nğŸ” Probando modelo {i+1}/{len(service.ollama_models)}: {model_name}")
        
        try:
            response = service._call_ollama(test_prompt, max_tokens=200, model_id=model_id)
            logger.info(f"âœ… {model_name} respondiÃ³:")
            logger.info(f"   {response[:150]}...")
        except Exception as e:
            logger.error(f"âŒ {model_name} fallÃ³: {e}")
            return False
    
    logger.info("\nâœ… Todos los modelos de Ollama funcionan correctamente")
    return True

def test_provider_fallback():
    """Prueba el fallback entre proveedores"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª TEST: FALLBACK ENTRE PROVEEDORES")
    logger.info("="*60)
    
    service = AIAnalyzerService()
    test_prompt = "Di solo: OK"
    
    try:
        text, provider = service._call_with_fallback_robust(test_prompt, max_tokens=50)
        logger.info(f"âœ… Respuesta obtenida de: {provider}")
        logger.info(f"   Texto: {text[:100]}")
        return True
    except Exception as e:
        logger.error(f"âŒ Fallback fallÃ³: {e}")
        return False

def test_all_providers():
    """Prueba todos los proveedores disponibles"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª TEST: TODOS LOS PROVEEDORES")
    logger.info("="*60)
    
    service = AIAnalyzerService()
    providers = service._get_provider_priority_list()
    
    logger.info(f"ğŸ“‹ Proveedores disponibles: {len(providers)}")
    for p in providers:
        logger.info(f"   - {p}")
    
    test_prompt = "Responde solo con: OK"
    
    for provider in providers[:3]:  # Probar solo los primeros 3
        logger.info(f"\nğŸ” Probando: {provider}")
        try:
            text, model = service._call_provider(provider, test_prompt, max_tokens=50)
            logger.info(f"âœ… {provider} funciona (modelo: {model})")
        except Exception as e:
            logger.warning(f"âš ï¸ {provider} fallÃ³: {e}")
    
    return True

if __name__ == "__main__":
    logger.info("\n" + "ğŸ§ª"*30)
    logger.info("INICIANDO TESTS DE CONEXIÃ“N IA")
    logger.info("ğŸ§ª"*30 + "\n")
    
    results = {
        "Ollama (3 modelos)": test_ollama_models(),
        "Fallback de proveedores": test_provider_fallback(),
        "Todos los proveedores": test_all_providers()
    }
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š RESUMEN DE TESTS")
    logger.info("="*60)
    
    for test_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        logger.info(f"{status} - {test_name}")
    
    all_passed = all(results.values())
    
    if all_passed:
        logger.info("\nğŸ‰ TODOS LOS TESTS PASARON")
    else:
        logger.error("\nâŒ ALGUNOS TESTS FALLARON")
    
    sys.exit(0 if all_passed else 1)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RESUMEN DE CAMBIOS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. âœ… Ollama agregado como PRIORIDAD 1 (antes de Gemini/OpenRouter/HuggingFace)
2. âœ… 3 modelos con fallback automÃ¡tico (Qwen â†’ DeepSeek â†’ Llama)
3. âœ… Auto-detecciÃ³n VPS vs PC local
4. âœ… Mantiene TODAS las APIs cloud como respaldo
5. âœ… Test suite completo para verificar funcionamiento

ORDEN DE PRIORIDAD FINAL:
1. ollama_0 (Qwen 2.5 7B) â† PRIMERO
2. ollama_1 (DeepSeek Coder)
3. ollama_2 (Llama 3.2 3B)
4. gemini â† Fallback cloud
5. openrouter
6. huggingface

VERIFICACIÃ“N:
python tests/test_ai_connection.py

DeberÃ­as ver todos los modelos respondiendo correctamente.